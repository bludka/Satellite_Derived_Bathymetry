{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document covers the transformation of ICESat-2 photon data over a target reef to depth estimates that will be used to calibrate Sentinel-2 bathymetry estimates.  The relevant program files are located in the <i><b>/src</i></b> directory in Karan Sunil's \"Coral-Reef-Bathymetry\" distribution on GitHub: https://github.com/karans04/Coral-Reef-Bathymetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, make sure this notebook is located and launched from a working directory containing <i><b>Depth_profile.py</i></b> and the other python files in Karan's distribution. After the imports below, any function available in <i><b>Depth_profile.py</i></b> can be called from this notebook with the syntax: depth.function_name(parameters). The same goes for the other imported python files.\n",
    "\n",
    "Note: sentinelsat is a package external to Anaconda3.  Downloaded the package from ESA, unzip, cd into it, then install into the base environment using /opt/anaconda3/bin/pip3 install sentinelsat or into a virtual environment using /opt/anaconda3/envs/[env name]/bin/pip3 install sentinelsat.\n",
    "\n",
    "Also, to get Beautiful Soup to work, I had to install the lxml library from the command line: conda install --name rasteriok lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import h5py\n",
    "import importlib\n",
    "import json\n",
    "import lxml\n",
    "import math\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyproj as proj\n",
    "import rasterio\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from scipy.stats import pearsonr\n",
    "from sentinelsat import SentinelAPI, read_geojson, geojson_to_wkt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from shapely.geometry import Point\n",
    "from tqdm import tqdm\n",
    "\n",
    "import Coral_Reef as coral_reef\n",
    "#import ATL03_API as is2_api\n",
    "import Depth_profile as depth\n",
    "import ICESat2_plots as is2_plot\n",
    "import ICESat2_file as is2_file\n",
    "import Sentinel_API as sentinel\n",
    "import Sentinel2_image as s2_img\n",
    "import Sentinel_plots as s2_plot\n",
    "import Reef_plots as reef_plots\n",
    "import Pixel_transformation as pt\n",
    "import Tide_API as tide\n",
    "import Water_level as water_level\n",
    "\n",
    "import icepyx as ipx\n",
    "import shutil\n",
    "\n",
    "%matplotlib inline\n",
    "import mpld3\n",
    "mpld3.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Input metadata for reef processing and initialize Coral_Reef data object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[run_bathymetry.py] <p>\n",
    "In Karan's top-level processing script <i><b>run_bathymetry.py</i></b>, the reef processing parameters are taken from <i><b>metadata/[reef_name]_params.json</i></b>, which we create below using the specified parameters. Separately, place the reef outline GeoJSON file described below into the metadata directory for this reef:\n",
    "<blockquote><b>A.)</b> a GeoJSON file containing the outline of the reef, named <i><b>[reef_name].geojson</i></b>.  This file is obtained from http://geojson.io/#map=2/20.0/0.0.  Switch to \"OSM\" or \"Satellite\" in bottom left corner of window (whatever shows reef best), select \"polygon\" mode at top-right, mark the boundary of reef with the cursor, save points using the menu at upper left (save->GeoJSON), then rename map.geojson file and move to data directory.</blockquote>\n",
    "Then create a Coral_Reef object to hold metadata for each reef. Take a look in Coral_Reef.py for the methods and variables defined for the Coral_Reef class. Also note that the get_bounding_box( ) method queries <i><b>/metadata/[reef_name].geojson</i></b> to calculate the reef bounding box coordinates... so not sure why we need to include the bounds in <i><b>/metadata/[reef_name].txt</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare project\n",
    "reef_name = 'yap'\n",
    "working_directory = '/Users/bonnieludka/Spaceship/IceSAT2/analysis_distribute/projects/'\n",
    "metadata_directory = os.path.join(working_directory, reef_name, 'metadata')\n",
    "\n",
    "#Processing Metadata\n",
    "sentinel_start_date = '20190101'\n",
    "sentinel_end_date = '20210430'#'20210505'#'20200601'\n",
    "download_is2 = False  #False = use the data already downloaded \n",
    "download_sentinel = False  #False = use the data already downloaded\n",
    "earthdata_username = 'bludka'\n",
    "earthdata_email = 'bludka@ucsd.edu'\n",
    "earthdata_password = 'Gnidlaps10!'\n",
    "sentinel_cloud_cover_percentage = [0, 10] #[min%, max%]\n",
    "sentinel_username = 'bludka'\n",
    "sentinel_password = 'Gnidlaps10!'\n",
    "world_tide_API_key = 'fee8ff39-48eb-42a7-bcc5-3819fce3c1e4'\n",
    "\n",
    "#Write Metadata\n",
    "metadata_file = os.path.join(metadata_directory, reef_name + '_params.json')\n",
    "metadata = {\n",
    "    'reef_name': reef_name,\n",
    "    'sentinel_start_date': sentinel_start_date,\n",
    "    'sentinel_end_date': sentinel_end_date,\n",
    "    'download_is2': download_is2,\n",
    "    'download_sentinel': download_sentinel,\n",
    "    'earthdata_username': earthdata_username,\n",
    "    'earthdata_password': earthdata_password,\n",
    "    'sentinel_username': sentinel_username,\n",
    "    'sentinel_password': sentinel_password,\n",
    "    'world_tide_API_key': world_tide_API_key,\n",
    "    'sentinel_cloud_cover_percentage': sentinel_cloud_cover_percentage,\n",
    "    'num_sentinel_imgs': 0    \n",
    "}\n",
    "with open(metadata_file, 'w') as json_file:\n",
    "  json.dump(metadata, json_file, indent = 4)\n",
    "\n",
    "#Initialize Coral_Reef data object\n",
    "reef = coral_reef.Coral_Reef(reef_name, working_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a.) Download Sentinel-2 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sentinel_API.py/get_sentinel_images()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download = 1\n",
    "\n",
    "if download == 1:\n",
    "\n",
    "    #Set data path\n",
    "    sentinel_path = reef.get_sentinel_rawdata_path()\n",
    "\n",
    "    #Load geojson of reef\n",
    "    reef_gjson_fp = os.path.join(reef.get_metadata_dir(), reef.get_reef_name()+'.geojson')\n",
    "    reef_gjson = read_geojson(reef_gjson_fp)\n",
    "    #Make sure longitude is within proper bounds\n",
    "    coords = reef_gjson.features[0].geometry.coordinates[0]\n",
    "    for i in range(len(coords)):\n",
    "        coord = coords[i]\n",
    "        coord[0] %= 360\n",
    "    gsf = open(reef_gjson_fp, \"w\")\n",
    "    json.dump(reef_gjson, gsf)\n",
    "    gsf.close()\n",
    "    reef_footprint = geojson_to_wkt(reef_gjson)\n",
    "\n",
    "    #Query archive for relevant S2 scenes \n",
    "    api = SentinelAPI(sentinel_username, sentinel_password, 'https://scihub.copernicus.eu/dhus')\n",
    "    products = api.query(reef_footprint,date = (sentinel_start_date, sentinel_end_date),\\\n",
    "            platformname = 'Sentinel-2', area_relation = 'Intersects', processinglevel = 'Level-2A',\\\n",
    "            cloudcoverpercentage = sentinel_cloud_cover_percentage, order_by = 'cloudcoverpercentage')  \n",
    "    print('Number of scenes: {}'.format(len(products)))\n",
    "\n",
    "    #Download files that are not yet local\n",
    "    for i,x in enumerate(products.items()):\n",
    "        k,v = x[0],x[1]\n",
    "        safe_folder = os.path.join(sentinel_path, v['title'] + '.SAFE')\n",
    "        if not os.path.exists(safe_folder):\n",
    "            print('Downloading: {}'.format(safe_folder))\n",
    "            api.download(k, directory_path = sentinel_path)\n",
    "        else:\n",
    "            print('{} exists!'.format(safe_folder))\n",
    "\n",
    "    #Unzip files\n",
    "    for file in os.listdir(sentinel_path):\n",
    "        if file.endswith('.zip'):\n",
    "            file_path = os.path.join(sentinel_path, file)\n",
    "            out_path = os.path.join(sentinel_path, file.split('.')[0])\n",
    "\n",
    "            if os.path.exists(file_path) and not os.path.exists(out_path):\n",
    "                with zipfile.ZipFile(file_path,\"r\") as zip_ref:\n",
    "                    zip_ref.extractall(sentinel_path)\n",
    "                os.remove(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b.) Extract Sentinel-2 image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pixel_transformation.py/all_safe_files(reef)]<p>\n",
    "Band 02 = Blue, Band 03 = Green, Band 04 = Red, Band 08 = Infrared.  All are 10 m resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "importlib.reload(s2_plot)\n",
    "importlib.reload(s2_img)\n",
    "\n",
    "#Set local parameters (these are set above, but repeated here)\n",
    "reef_path = reef.get_reef_dir()\n",
    "reef_name = reef.get_reef_name()\n",
    "sentinel_path = reef.get_sentinel_rawdata_path()\n",
    "print(sentinel_path)\n",
    "coords = reef.get_bounding_box()\n",
    "print(coords)\n",
    "\n",
    "#Choose Sentinel file for analysis\n",
    "sf = os.listdir(sentinel_path)\n",
    "sf = [f for f in sf if f.endswith('.SAFE')]\n",
    "print(sf)\n",
    "print('hi')\n",
    "\n",
    "#NOTE: THIS IS WHERE WE CHOOSE WHICH SENTINEL FILE TO READ IN\n",
    "sf = sf[0] \n",
    "print(sf)\n",
    "\n",
    "#Load Sentinel file into data object\n",
    "sf_path = os.path.join(sentinel_path, sf)\n",
    "safe_file = s2_img.Sentinel2_image(reef, sf_path, coords)\n",
    "\n",
    "#Extract the Sentinel metadata and bands\n",
    "meta = safe_file.get_meta()   #this has metadata in UTM coords. MAYBE CAN SEPARATE IMAGES FROM METADATA\n",
    "imgs = meta['imgs']\n",
    "\n",
    "#Create infrared threshold value for masking land/clouds\n",
    "b8_flat = np.ndarray.flatten(imgs[3])  #make list of all Band 8 pixel values\n",
    "b8_robust_sigma = np.subtract(*np.percentile(b8_flat, [75, 25]))/1.349\n",
    "mask_thresh = np.median(b8_flat) + 2*b8_robust_sigma  #threshold is median val + 2*sigma\n",
    "safe_file.meta['mask_thresh'] = mask_thresh\n",
    "print('B8: robust sigma {0:5.2f}  mask threshold {1:5.2f}'.format(b8_robust_sigma, mask_thresh))\n",
    "\n",
    "#Plot sentinel data\n",
    "s2_plot.plot_s2_band_raw(meta)\n",
    "s2_plot.plot_s2_band_histogram(meta)\n",
    "s2_plot.plot_s2_band_diff(meta)\n",
    "s2_plot.plot_s2_band_diff_histogram(meta)\n",
    "s2_plot.plot_s2_band_vs_band(meta)\n",
    "#s2_plot.plot_s2_band_ratio(meta) #test of log(B2)/log(B3) [doesn't help]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.) Get ICESat-2 data for reef using icepyx https://github.com/icesat2py/icepyx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- [run_bathymetry.py] <p>\n",
    "This requires running <i><b>/code/ATL03_API.main()</i></b>, which we are not going to do for this example. ICESat-2 HDF5 files will be downloaded into the <i><b>data/is2_photons</i></b> folder, and optionally those with little/bad data will be culled and put into <i><b>is2_photons_bad</i></b> folder.\n",
    "<p>The ICESat-2 ATL03 data files are HDF5 files that can be obtained manually from OpenAltimetry (http://www.openaltimetry.org) or from NASA EarthData search (https://search.earthdata.nasa.gov/search/granules?p=C1705401930-NSIDC_ECS) -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# put in for loop to decide whether need to download\n",
    "\n",
    "# specify date range\n",
    "# use same start/end dates as sentinel dates (or can specify other dates. date format is 'yyyy-mm-dd' string)\n",
    "is2_start_date = sentinel_start_date[0:4]+'-'+sentinel_start_date[4:6]+'-'+sentinel_start_date[6:8]\n",
    "is2_end_date = sentinel_end_date[0:4]+'-'+sentinel_end_date[4:6]+'-'+sentinel_end_date[6:8]\n",
    "date_range = [is2_start_date,is2_end_date]\n",
    "\n",
    "# specify spatial extent\n",
    "# use reef bounds\n",
    "reef_gjson_fp = os.path.join(reef.get_metadata_dir(), reef.get_reef_name()+'.geojson')\n",
    "reef_gjson = read_geojson(reef_gjson_fp)\n",
    "coords = reef_gjson.features[0].geometry.coordinates[0]\n",
    "spatial_extent = [] \n",
    "for i in range(len(coords)):\n",
    "    coord = coords[i]\n",
    "    lon = coord[0]\n",
    "    lat = coord[1]\n",
    "    spatial_extent.append((lon,lat))\n",
    "#print(spatial_extent)\n",
    "# Hrm..the complex spatial extent seems to have issues\n",
    "# https://github.com/ICESat2-SlideRule/sliderule-python/issues/24\n",
    "# https://github.com/icesat2py/icepyx/issues/159\n",
    "# Use square bounding box for now but this is not an all the time fix (will break under some scenarios I think)\n",
    "se = np.array(spatial_extent)\n",
    "lon = se[:,0]\n",
    "lat = se[:,1]\n",
    "box_around_spatial_extent = [np.min(lon), np.min(lat), np.max(lon), np.max(lat)]\n",
    "print(box_around_spatial_extent)\n",
    "\n",
    "# create query for specified spatial extent and date range\n",
    "#region = ipx.Query('ATL03', spatial_extent, date_range) # grab data within complex polygon matching reef outline\n",
    "region = ipx.Query('ATL03', box_around_spatial_extent, date_range) # grab data within square bounding box\n",
    "region.visualize_spatial_extent()\n",
    "print(region.CMRparams)\n",
    "print(region.avail_granules())\n",
    "print(region.avail_granules(ids=True))\n",
    "\n",
    "# log in\n",
    "earthdata_email = 'bludka@ucsd.edu'\n",
    "region.earthdata_login(earthdata_username, earthdata_email)\n",
    "\n",
    "# request parameters\n",
    "# these come recommended: https://github.com/icesat2py/icepyx/blob/main/examples/ICESat-2_DAAC_DataAccess_Example.ipynb\n",
    "region.reqparams['page_size'] = 10\n",
    "region.reqparams['page_num'] = 1\n",
    "region.reqparams['request_mode'] = 'async'\n",
    "#region.reqparams['agent'] = 'NO' # This threw an error\n",
    "region.reqparams['include_meta'] = 'Y'\n",
    "print(region.reqparams)\n",
    "\n",
    "# subset variables to extract in query\n",
    "region.order_vars.append(var_list=['h_ph','lat_ph','lon_ph','signal_conf_ph','ref_azimuth','ref_elev'])\n",
    "#print(region.order_vars.wanted)\n",
    "region.subsetparams(Coverage=region.order_vars.wanted)\n",
    "\n",
    "# place order\n",
    "region.order_granules()\n",
    "h5_dir = reef.get_icesat_rawdata_path()\n",
    "region.download_granules(h5_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(coords)\n",
    "print(' ')\n",
    "print(spatial_extent)\n",
    "region = ipx.Query('ATL03', spatial_extent, date_range)\n",
    "region.visualize_spatial_extent()\n",
    "print(region.CMRparams)\n",
    "blah = region.CMRparams['polygon']\n",
    "#print(blah)\n",
    "#print(' ')\n",
    "blee = np.array([[138.133163,9.631553],[138.032913,9.417565],[138.204918,9.527623],[138.133163,9.631553]])\n",
    "lon = blee[:,0]\n",
    "#print('lon')\n",
    "#print(lon)\n",
    "lat = blee[:,1]\n",
    "#print('lat')\n",
    "#print(lat)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(lon,lat,'ro-')\n",
    "#print(' ')\n",
    "#print(spatial_extent)\n",
    "se = np.array(spatial_extent)\n",
    "#print(se)\n",
    "lon = se[:,0]\n",
    "#print('lon')\n",
    "#print(lon)\n",
    "lat = se[:,1]\n",
    "#print('lat')\n",
    "#print(lat)\n",
    "plt.plot(lon,lat,'bo-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.) Read photon data, calculate photon depths relative to sea surface, save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[run.py / Depth_profile.py / get_depths(reef) / process_H5()]<p>\n",
    "Assuming for now that the ICESat-2 data have been downloaded and dropped into the <i><b>H5</i></b> directory.\n",
    "The depth.process_H5(reef) method will run the entire depth estimation workflow. Here it is line-by-line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "remake_is2_bathymetry = 1\n",
    "\n",
    "#importlib.reload(depth)\n",
    "\n",
    "if remake_is2_bathymetry == 1:\n",
    "    #Get IS2 file directory and filenames\n",
    "    h5_dir = reef.get_icesat_rawdata_path()\n",
    "    h5_filenames = [f for f in os.listdir(h5_dir) if not f.startswith('.')]\n",
    "    h5_filenames.sort()\n",
    "    \n",
    "    for h5_fn in h5_filenames:\n",
    "        print('\\n' + h5_fn)\n",
    "\n",
    "        #Inititalize ICESat-2 file object\n",
    "        is2 = is2_file.IS2_file(reef, h5_fn)\n",
    "        is2_file_tag = is2.get_file_tag()\n",
    "        strong_beams = is2.get_strong_lasers()\n",
    "        print(strong_beams)\n",
    "\n",
    "        for laser in strong_beams:\n",
    "\n",
    "            #Output directory for csv file containing raw photon data\n",
    "            photon_fn = '{reef_name}_photons_{h5_fn}_{laser}.csv'.format(reef_name=reef_name, h5_fn=is2_file_tag, laser=laser)\n",
    "            photons_path = os.path.join(reef.get_icesat_photons_path(), photon_fn)\n",
    "\n",
    "            #Load raw photon data from .CSV if it already exists, else extract data from subsetted IS2 H5 file\n",
    "            if not os.path.exists(photons_path):\n",
    "\n",
    "                #Create dataframe with photon data from subsetted IS2 H5 file\n",
    "                photon_data = is2.get_photon_data(laser)  \n",
    "                if len(photon_data) == 0:\n",
    "                    print('No photons in file for {}'.format(laser))\n",
    "                    continue            \n",
    "                df_laser = depth.create_photon_df(photon_data)\n",
    "\n",
    "                #Clip dataframe to reef bounding shape\n",
    "                reef_polygon = reef.return_reef_polygon()\n",
    "                gdf = gpd.GeoDataFrame(df_laser, geometry=gpd.points_from_xy(df_laser.Longitude, df_laser.Latitude))\n",
    "                poly_gdf = gpd.GeoDataFrame([1], geometry=[reef_polygon])\n",
    "                gdf_clipped = gpd.clip(gdf, poly_gdf)\n",
    "                df = pd.DataFrame(gdf_clipped)\n",
    "\n",
    "                #Optional: clip photons to just high-confidence photons\n",
    "                df = df.loc[(df.Conf_ocean == 4) | (df.Conf_land == 4)].copy() #(df.Conf_ocean == 3) | (df.Conf_ocean == 1) | \\\n",
    "                             #| (df.Conf_land == 3) | (df.Conf_land == 1)].copy()\n",
    "\n",
    "                #If there are photons in the new clipped dataframe\n",
    "                if len(df) != 0:    \n",
    "\n",
    "                    #Adjust elevations to reference local sea level (tide correction comes later)\n",
    "                    #Unlike Karan's code, df now contains 'sea_level', which has been removed from 'Heights'\n",
    "                    df,f = water_level.normalise_sea_level(df)\n",
    "                    #is2_plot.plot_is2_depths(df, is2, laser, f, [0, 55], 'Raw Photon Elevs')\n",
    "                    if len(df) == 0:  \n",
    "                        print('Failed at sea level parameterization for {}'.format(laser))\n",
    "                        continue\n",
    "                    is2.set_sea_level_function(f,laser)\n",
    "\n",
    "                    #Adjust depth for speed of light in water\n",
    "                    df = water_level.adjust_for_speed_of_light_in_water(df)\n",
    "\n",
    "                    #Write a dataframe containing just the photon data in bounding box that meet the confidence criterion\n",
    "                    df.to_csv(photons_path)\n",
    "                else:\n",
    "                    print('No photons over reef for {}'.format(laser))\n",
    "\n",
    "            else:\n",
    "                print('\\n' + 'Reading photons from prior csv file for {}'.format(laser))\n",
    "                df = pd.read_csv(photons_path)\n",
    "                is2.metadata = is2.load_json()\n",
    "\n",
    "            print('Number photons in {laser} is {reef_length}'.format(laser=laser, reef_length=str(len(df))))       \n",
    "\n",
    "            #Classify seafloor photons\n",
    "            eps_val = 0.75\n",
    "            min_samp = 6\n",
    "            water_thresh_low = -50\n",
    "            water_thresh_high = -0.4\n",
    "            bathy, non_bathy = depth.apply_DBSCAN(df, eps_val, min_samp, water_thresh_low, water_thresh_high, safe_file)\n",
    "\n",
    "            if len(bathy) == 0:\n",
    "                print('No bathymetric photons found for {}'.format(laser))\n",
    "                continue\n",
    "            print('Number bathy photons in {laser} is {reef_length}'.format(laser=laser, reef_length=str(len(bathy))))\n",
    "\n",
    "            #Output just bathymetric photons to new .CSV file\n",
    "            depths_fn = '{reef_name}_{h5_fn}_{laser}.csv'.format(reef_name=reef_name,h5_fn=is2_file_tag,laser=laser)\n",
    "            bathymetry_output_path = os.path.join(reef.get_icesat_bathymetry_path(), depths_fn)\n",
    "            bathy.to_csv(bathymetry_output_path)\n",
    "            \n",
    "            #Plot depths\n",
    "            is2_plot.plot_is2_depths_bathy(df, is2, laser, bathy, reef.get_icesat_images_path(), 'False', [-25, 5], 'Corrected Bathymetric Profile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.) Get Training (Calibration) bathymetry data from ICESat-2 and Sentinel-2\n",
    "[Pixel_transformation.py/get_regressor(reef, sf)]<br>\n",
    "[Pixel_transformation.py/load_ICESAT_predictions(icesat_proc_path,sf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pt)\n",
    "importlib.reload(reef_plots)\n",
    "importlib.reload(s2_plot)\n",
    "\n",
    "#Get list of all ICESat bathymetry files\n",
    "icesat_bathymetry_path = reef.get_icesat_bathymetry_path()\n",
    "is2files = [file for file in os.listdir(icesat_bathymetry_path) if file.endswith('.csv')]\n",
    "\n",
    "#Define and populate ICESat training (calibration) dataframe; calculate and apply tides in pt.prep_df\n",
    "train_raw = pd.DataFrame()\n",
    "for fn in is2files:\n",
    "    train_path = os.path.join(icesat_bathymetry_path, fn)\n",
    "    #pt.prep_df gets tides and corrects IS2 training bathymetry to depth valid for Sentinel-2 image date\n",
    "    train_raw = pd.concat([train_raw, pt.prep_df(safe_file, train_path, safe_file.get_crs(),reef)])   \n",
    "train_raw = train_raw.dropna()\n",
    "\n",
    "#Define functions we need below\n",
    "def get_pixel_val(coord):\n",
    "    \"\"\"\n",
    "    Get pixel value given a set of coordinates\n",
    "    Params - 1. coord (Point) - point of interest\n",
    "    Return int - pixel value at point\n",
    "    \"\"\"\n",
    "    x_index = int((coord.x - meta['ulx']) // meta['xdim'])\n",
    "    y_index = int((coord.y - meta['uly']) // (meta['ydim']))\n",
    "    return [data[0][y_index][x_index] for data in imgs]\n",
    "\n",
    "def get_pixel_image_coords(coord):\n",
    "    \"\"\"\n",
    "    Get pixel value given a set of coordinates\n",
    "    Params - 1. coord (Point) - point of interest\n",
    "    Return int - pixel value at point\n",
    "    \"\"\"\n",
    "    x_index = int((coord.x - meta['ulx']) // meta['xdim'])\n",
    "    y_index = int((coord.y - meta['uly']) // (meta['ydim']))\n",
    "    return y_index, x_index\n",
    "\n",
    "def extract_pixel_cols(df):\n",
    "    \"\"\"\n",
    "    Extracts band values for image\n",
    "    Params - 1. df (DataFrame) - depth predictions of ICESAT-2\n",
    "    Return - DataFrame - pixel values added for each point\n",
    "    \"\"\"\n",
    "    df['Pixels'] = df.Coordinates.apply(get_pixel_val)\n",
    "    df['b2'] = df.Pixels.apply(lambda x: (x[0]))\n",
    "    df['b3'] = df.Pixels.apply(lambda x: (x[1]))\n",
    "    df['b4'] = df.Pixels.apply(lambda x: (x[2]))\n",
    "    df['b8'] = df.Pixels.apply(lambda x: (x[3]))\n",
    "    return df\n",
    "\n",
    "#Get Sentinel band values at IS2 pixel locations\n",
    "train = train_raw.copy(deep=True)  #first make a copy of \"train_raw\" so it can be altered for each S2 image\n",
    "train = extract_pixel_cols(train)\n",
    "\n",
    "#Plot histogram and print statistics of Sentinel calibration pixels before masking\n",
    "print('')\n",
    "print('Training pixel stats before masking land/clouds')\n",
    "print('B2: min {0:5d}, max {1:5d}, mean {2:6.1f}, median {3:6.1f}'.format(min(train['b2']), max(train['b2']), np.mean(train['b2']), np.median(train['b2'])))\n",
    "print('B3: min {0:5d}, max {1:5d}, mean {2:6.1f}, median {3:6.1f}'.format(min(train['b3']), max(train['b3']), np.mean(train['b3']), np.median(train['b3'])))\n",
    "print('B4: min {0:5d}, max {1:5d}, mean {2:6.1f}, median {3:6.1f}'.format(min(train['b4']), max(train['b4']), np.mean(train['b4']), np.median(train['b4'])))\n",
    "print('B8: min {0:5d}, max {1:5d}, mean {2:6.1f}, median {3:6.1f}'.format(min(train['b8']), max(train['b8']), np.mean(train['b8']), np.median(train['b8'])))\n",
    "reef_plots.band_histogram(train)\n",
    "\n",
    "#Apply land/cloud mask based on Band 8 (infrared)\n",
    "print('')\n",
    "print('Mask threshold: ', mask_thresh)\n",
    "train['mask'] = train.b8.apply(lambda x: False if x < mask_thresh else True)\n",
    "print('N photons before edits: ', len(train))\n",
    "train = train.loc[(train.b2 != 0) & (train.b3 != 0)].copy()\n",
    "print('N non-zero photons: ', len(train))\n",
    "#train = train.loc[(train.b2 != 0) & (train.b3 != 0) & (train.b4 != 0)] #use this if training with Band 04 (red)\n",
    "train = train.loc[train['mask'] == False].copy()\n",
    "print('N Band8-masked non-zero photons: ',len(train))\n",
    "print('')\n",
    "\n",
    "#Plot histogram and print statistics of Sentinel calibration pixels after masking\n",
    "print('')\n",
    "print('Training pixel stats after removing zeros and masking land/clouds')\n",
    "print('B2: min {0:5d}, max {1:5d}, mean {2:6.1f}, median {3:6.1f}'.format(min(train['b2']), max(train['b2']), np.mean(train['b2']), np.median(train['b2'])))\n",
    "print('B3: min {0:5d}, max {1:5d}, mean {2:6.1f}, median {3:6.1f}'.format(min(train['b3']), max(train['b3']), np.mean(train['b3']), np.median(train['b3'])))\n",
    "print('B4: min {0:5d}, max {1:5d}, mean {2:6.1f}, median {3:6.1f}'.format(min(train['b4']), max(train['b4']), np.mean(train['b4']), np.median(train['b4'])))\n",
    "print('B8: min {0:5d}, max {1:5d}, mean {2:6.1f}, median {3:6.1f}'.format(min(train['b8']), max(train['b8']), np.mean(train['b8']), np.median(train['b8'])))\n",
    "reef_plots.band_histogram(train)\n",
    "\n",
    "#Ensure that no training pixels are zero or negative\n",
    "delta = 0.000\n",
    "bp = {'B02': delta, 'B03': delta, 'B04': delta}\n",
    "safe_file.meta['min_pix'] = bp\n",
    "train['b2'] = train['b2'].apply(lambda x: max(delta,x - bp['B02']))\n",
    "train['b3'] = train['b3'].apply(lambda x: max(delta,x - bp['B03']))\n",
    "train['b4'] = train['b4'].apply(lambda x: max(delta,x - bp['B04']))\n",
    "\n",
    "#Get coordinates of training data for overplotting on sentinel image\n",
    "train['x'] = train.Coordinates.x\n",
    "train['y'] = train.Coordinates.y\n",
    "train['image_coords'] = train.Coordinates.apply(get_pixel_image_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.) Calibrate Sentinel-2 depth estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(reef_plots)\n",
    "#mpld3.disable_notebook()\n",
    "mpld3.enable_notebook()\n",
    "\n",
    "#Calculates the log difference between band pixels\n",
    "train['log_b2'] = np.log(train['b2'])\n",
    "train['log_b3'] = np.log(train['b3'])\n",
    "train['log_b4'] = np.log(train['b4'])\n",
    "train['diff_b2_b3'] = train.eval('b2-b3')\n",
    "train['diff_b2_b4'] = train.eval('b2-b4')\n",
    "train['diff_b3_b4'] = train.eval('b3-b4')\n",
    "\n",
    "#Optionally edit data outliers\n",
    "train_cleaned = train.copy(deep=True)  #first make a copy of \"train\" so it can be altered for each S2 image\n",
    "#train_cleaned = pt.remove_log_outliers(train_cleaned)\n",
    "\n",
    "#Relationships between calibration bathymetry and log band differences\n",
    "x = train_cleaned.loc[:,['Height']]\n",
    "y = train_cleaned.loc[:,['diff_b2_b3']]\n",
    "m, c = np.polyfit(np.ravel(x), np.ravel(y), 1) #m = slope, c=intercept\n",
    "y2 = train_cleaned.loc[:,['diff_b3_b4']]\n",
    "m2, c2 = np.polyfit(np.ravel(x), np.ravel(y2), 1) #m = slope, c=intercept\n",
    "print('Slope/Intercept Blue-Green: {0:6.3f}/{1:6.3f}'.format(m, c))\n",
    "print('Slope/Intercept Green-Red : {0:6.3f}/{1:6.3f}'.format(m2, c2))\n",
    "\n",
    "#Plot calibration results\n",
    "reef_plots.plot_sentinel_cal(train_cleaned, m, c, m2, c2)\n",
    "\n",
    "#Return training results for Blue/Green\n",
    "line = lambda x: (x-c)/m\n",
    "out = train_cleaned[['x','y','b2','b3','diff_b2_b3','Height']]\n",
    "#Return training results for Green/Red\n",
    "#line = lambda x: (x-c2)/m2  \n",
    "#out = train_cleaned[['x','y','b3','b4','diff_b3_b4','Height']] \n",
    "\n",
    "#Output training file\n",
    "training_data_out_fn = '{reef_name}_training_data_{date}.csv'.\\\n",
    "    format(reef_name = reef_name, date = safe_file.get_date().strftime(\"%Y%m%d%H%M%S\"))\n",
    "train_cleaned.to_csv(os.path.join(reef.get_training_data_path(), training_data_out_fn))\n",
    "\n",
    "#Plot location of training data on reef\n",
    "reef_plots.plot_sentinel_icesat(imgs, meta, train_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.) Get reef depths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixel_transformation.py/predict_reef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_reef(reg, sf,master_df):\n",
    "# called as: preds,master_df = predict_reef(line, safe_file, master_df)\n",
    "#\n",
    "#    Predict the depth of the reef using colour pixel values\n",
    "#    Params - 1. reg (lambda) - predict depth using pixel values\n",
    "#             2. sf (Sentinel2_image) - Object representing sentinel image\n",
    "#             3. master_df (DataFrame) - contains depth predictions from all sentinel images\n",
    "#    Return - str - path of out file\n",
    "#           - DataFrame - containing depth predictions from all sentinel images\n",
    "\n",
    "#create lists/dataframe to store required values\n",
    "x, y, height, pix, diff_log = [],[],[],[],[]\n",
    "master_df = pd.DataFrame()\n",
    "\n",
    "#restore parameters\n",
    "meta = safe_file.get_meta()\n",
    "mask_thresh = meta['mask_thresh']\n",
    "tide_level = safe_file.get_tide()\n",
    "bbox_coords = safe_file.read_gjson()['geometry'][0].bounds\n",
    "print(mask_thresh, tide_level, bbox_coords)\n",
    "\n",
    "#loads in the required images\n",
    "b2_pix, b3_pix, b4_pix, b8_pix = meta['imgs']\n",
    "print(b2_pix.shape, b3_pix.shape, b4_pix.shape, b8_pix.shape)\n",
    "time.sleep(1)\n",
    "\n",
    "#loop through the image coordinates\n",
    "for j in tqdm(range(len(b2_pix[0]))):\n",
    "    for i in range(len(b2_pix[0][0])):\n",
    "        b2, b3, b4, b8 = b2_pix[0][j][i], b3_pix[0][j][i], b4_pix[0][j][i], b8_pix[0][j][i]\n",
    "        if b2 and b3:\n",
    "        #if b3 and b4: #use for Green minus Red analysis\n",
    "            \n",
    "            #generate x,y coordinate for pixel\n",
    "            x_coord = bbox_coords[0] + ((i)*meta['xdim']) + 5\n",
    "            y_coord = bbox_coords[3] + ((j)*meta['ydim']) - 5\n",
    "            x.append(x_coord)\n",
    "            y.append(y_coord)\n",
    "            #p = Point((x_coord, y_coord))\n",
    " \n",
    "            #get the normalised band values\n",
    "            bp = meta['min_pix']\n",
    "            delta = 0.0\n",
    "            band_2 = max(delta, b2 - bp['B02'])\n",
    "            band_3 = max(delta, b3 - bp['B03'])\n",
    "            band_4 = max(delta, b4 - bp['B03'])\n",
    "            band_8 = b8\n",
    "            pix.append([band_2, band_3, band_4, band_8])\n",
    "           \n",
    "            #if the band 8 value is higher than the threshold we predict, use nan for the height, else the depth adjust with the tide\n",
    "            if band_8 > mask_thresh:\n",
    "                height.append(np.nan)\n",
    "                diff_log.append(np.nan)\n",
    "            else:\n",
    "                diff = math.log(band_2) - math.log(band_3)\n",
    "                #diff = math.log(band_3) - math.log(band_4) #use for Green minus Red analysis\n",
    "                diff_log.append(diff)\n",
    "                pred = (line(diff) - tide_level)\n",
    "                height.append(pred)\n",
    "\n",
    "#create a dataframe with the output information \n",
    "reef_depths = pd.DataFrame([x, y, height, pix, diff_log]).T\n",
    "reef_depths.columns = ['x', 'y', 'Height', 'normalised_pixel', 'diff']\n",
    "\n",
    "#Save df as a csv\n",
    "dt = safe_file.get_date()\n",
    "out_fn = '{reef_name}_out_{dt}.csv'.format(reef_name = reef.reef_name, dt = dt.strftime(\"%Y%m%d%H%M%S\"))\n",
    "out_fp = os.path.join(reef.get_sentinel_depths_path(), out_fn)\n",
    "reef_depths.to_csv(out_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot reef depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(reef_plots)\n",
    "\n",
    "reef_plots.plot_reefs(reef_depths, train_cleaned, safe_file, line, reef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
