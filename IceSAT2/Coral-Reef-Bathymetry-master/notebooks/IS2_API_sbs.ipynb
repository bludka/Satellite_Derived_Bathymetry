{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import getpass\n",
    "import socket \n",
    "import json\n",
    "import zipfile\n",
    "import io\n",
    "import math\n",
    "import os \n",
    "import shutil\n",
    "import time\n",
    "import h5py\n",
    "import re\n",
    "import requests\n",
    "import bs4\n",
    "import subprocess\n",
    "import numpy\n",
    "import base64\n",
    "# To read KML files with geopandas, we will need to enable KML support in fiona (disabled by default)\n",
    "import pprint\n",
    "from statistics import mean\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from xml.etree import ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from urllib.parse import urlparse\n",
    "    from urllib.request import urlopen, Request, build_opener, HTTPCookieProcessor\n",
    "    from urllib.error import HTTPError, URLError\n",
    "except ImportError:\n",
    "    from urlparse import urlparse\n",
    "    from urllib2 import urlopen, Request, HTTPError, URLError, build_opener, HTTPCookieProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earthdata Login password: ········\n"
     ]
    }
   ],
   "source": [
    "uid = 'karans04'\n",
    "pswd = getpass.getpass('Earthdata Login password: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [422]>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-015b69f95b87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_api_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'token'"
     ]
    }
   ],
   "source": [
    "token_api_url = 'https://cmr.earthdata.nasa.gov/legacy-services/rest/tokens'\n",
    "hostname = socket.gethostname()\n",
    "ip = socket.gethostbyname(hostname)\n",
    "\n",
    "data = {\n",
    "    'token': {\n",
    "        'username': uid,\n",
    "        'password': pswd,\n",
    "        'client_id': 'NSIDC_client_id',\n",
    "        'user_ip_address': ip\n",
    "    }\n",
    "}\n",
    "\n",
    "headers={'Accept': 'application/json'}\n",
    "response = requests.post(token_api_url, json=data, headers=headers)\n",
    "print(response)\n",
    "token = json.loads(response.content)['token']['id']\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set API request parameters\n",
    "short_name = 'ATL03'\n",
    "dataset_version = '002'\n",
    "time_start = '2018-10-13T00:00:00Z'\n",
    "page_size = 100\n",
    "page_num = 1\n",
    "request_mode = 'async'\n",
    "\n",
    "# Bounding Box spatial parameter in 'W,S,E,N' format\n",
    "# Input lower left longitude in decimal degrees\n",
    "LL_lon = '-178.55'\n",
    "# Input lower left latitude in decimal degrees\n",
    "LL_lat = '-18.76'\n",
    "# Input upper right longitude in decimal degrees\n",
    "UR_lon = '-178.40'\n",
    "# Input upper right latitude in decimal degrees\n",
    "UR_lat = '-18.61'\n",
    "\n",
    "#Bounding box subsetting (bbox) in same format as bounding_box\n",
    "bounding_box = LL_lon + ',' + LL_lat + ',' + UR_lon + ',' + UR_lat\n",
    "bbox = bounding_box\n",
    "\n",
    "# bounding box input:\n",
    "search_params = {\n",
    "    'short_name': short_name,\n",
    "    'version': dataset_version,\n",
    "    'temporal': time_start,\n",
    "    'page_size': 100,\n",
    "    'page_num': 1,\n",
    "    'bounding_box': bounding_box,\n",
    "    }\n",
    "download_params = {\n",
    "    'short_name': short_name,\n",
    "    'version': dataset_version,\n",
    "    'temporal': time_start,\n",
    "     #'time': time_var, \n",
    "    'bounding_box': bounding_box,\n",
    "     'bbox': bbox, \n",
    "     #'format': reformat, \n",
    "     #'projection': projection, \n",
    "     #'projection_parameters': projection_parameters, \n",
    "     #'Coverage': coverage, \n",
    "    'page_size': 100,\n",
    "     #'page_num': 1,\n",
    "    'token': token,\n",
    "    'request_mode': request_mode, \n",
    "     #'agent': agent, \n",
    "#     'email': email,\n",
    "    }\n",
    "\n",
    "#Set Earthdata search URL\n",
    "granule_search_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n",
    "\n",
    "#Set NSIDC data access base URL\n",
    "base_url = 'https://n5eil02u.ecs.nsidc.org/egi/request'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check available versions\n",
    "params = {\n",
    "    'short_name': short_name\n",
    "}\n",
    "\n",
    "cmr_collections_url = 'https://cmr.earthdata.nasa.gov/search/collections.json'\n",
    "response = requests.get(cmr_collections_url, params=params)\n",
    "results = json.loads(response.content)\n",
    "\n",
    "# Find all instances of 'version_id' in metadata and print most recent version number\n",
    "\n",
    "versions = [el['version_id'] for el in results['feed']['entry']]\n",
    "latest_version = max(versions)\n",
    "print('The available versions of ', short_name, ' are ', versions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "granules = []\n",
    "headers={'Accept': 'application/json'}\n",
    "\n",
    "while True:\n",
    "    response = requests.get(granule_search_url, params=search_params, headers=headers)\n",
    "    results = json.loads(response.content)\n",
    "\n",
    "    if len(results['feed']['entry']) == 0:\n",
    "        # Out of results, so break out of loop\n",
    "        break\n",
    "\n",
    "    # Collect results and increment page_num\n",
    "    granules.extend(results['feed']['entry'])\n",
    "    search_params['page_num'] += 1\n",
    "\n",
    "print('There are', len(granules), 'granules of', short_name, 'version', dataset_version, 'over my area and time of interest.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(*granules, sep = \"\\n\") \n",
    "urls = []\n",
    "for granule in granules:\n",
    "    print(granule['links'][0]['href'])\n",
    "    urls.append(granule['links'][0]['href'])\n",
    "# Average/Total size of granules in MB\n",
    "granule_sizes = [float(granule['granule_size']) for granule in granules]\n",
    "print(sum(granule_sizes)/len(granules))\n",
    "print(sum(granule_sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create an output folder if the folder does not already exist.\n",
    "datadir = str(os.path.realpath('.') + '/data')\n",
    "if not os.path.exists(datadir):\n",
    "    os.mkdir(datadir)\n",
    "download_path = os.path.join(datadir,'is2')\n",
    "if not os.path.exists(download_path):\n",
    "    os.mkdir(download_path)\n",
    "print(download_path)\n",
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "credentials = '{0}:{1}'.format(uid,pswd)\n",
    "credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_count = len(urls)\n",
    "print('Downloading {0} files...'.format(url_count))\n",
    "for index, url in enumerate(urls):\n",
    "    filename = url.split('/')[-1]\n",
    "    print('{0}/{1}: {2}'.format(str(index).zfill(len(str(url_count))),\n",
    "                            url_count,\n",
    "                            filename))\n",
    "    if os.path.exists(download_path + '/' + filename):\n",
    "            continue\n",
    "    try:\n",
    "        # In Python 3 we could eliminate the opener and just do 2 lines:\n",
    "        # resp = requests.get(url, auth=(username, password))\n",
    "        # open(filename, 'wb').write(resp.content)\n",
    "        req = Request(url)\n",
    "        if credentials:\n",
    "            req.add_header('Authorization', 'Basic {0}'.format(credentials))\n",
    "        opener = build_opener(HTTPCookieProcessor())\n",
    "        data = opener.open(req).read()\n",
    "        open(download_path + '/' + filename, 'wb').write(data)\n",
    "    except HTTPError as e:\n",
    "        print('HTTP error {0}, {1}'.format(e.code, e.reason))\n",
    "    except URLError as e:\n",
    "        print('URL error: {0}'.format(e.reason))\n",
    "    except IOError:\n",
    "        raise\n",
    "    except KeyboardInterrupt:\n",
    "        quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
