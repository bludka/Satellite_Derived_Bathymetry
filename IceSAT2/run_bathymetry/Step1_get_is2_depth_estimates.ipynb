{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document covers the transformation of ICESat-2 photon data over a target reef to depth estimates that will be used to calibrate Sentinel-2 bathymetry estimates.  The relevant program files are located in the <i><b>/src</i></b> directory in Karan Sunil's \"Coral-Reef-Bathymetry\" distribution on GitHub: https://github.com/karans04/Coral-Reef-Bathymetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, make sure this notebook is located and launched from a working directory containing <i><b>Depth_profile.py</i></b> and the other python files in Karan's distribution. After the imports below, any function available in <i><b>Depth_profile.py</i></b> can be called from this notebook with the syntax: depth.function_name(parameters). The same goes for the other imported python files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import h5py\n",
    "import importlib\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyproj as proj\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import Coral_Reef as coral_reef\n",
    "import Depth_profile as depth\n",
    "import ICESAT_plots as is2_plot\n",
    "import IS2_file as is2File\n",
    "import Tide_API as tide\n",
    "import Water_level as water_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Create directories and files needed for depth processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In working directory, create a folder called <i><b>data</i></b> and create subfolders for each reef being analyzed.  Within each subfolder, provide the following folders/files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote><b>A.)</b> a GeoJSON file containing the outline of the reef, named <i><b>reef_name.geojson</i></b>.  This file is obtained from http://geojson.io/#map=2/20.0/0.0.  Switch to \"OSM model\" in bottom left corner of window, mark the boundary of reef with the cursor, save points using the menu at upper left (save->GeoJSON), then rename map.geojson file and move to data directory.\n",
    "<p>\n",
    "<b>B.)</b> a folder named <i><b>H5</i></b> which contains ICESat-2 ATL03 data files for this reef.  These are HDF5 files that can be obtained from OpenAltimetry (http://www.openaltimetry.org) or from NASA EarthData search (https://search.earthdata.nasa.gov/search/granules?p=C1705401930-NSIDC_ECS)\n",
    "<p>\n",
    "<b>C.)</b> a file called <i><b>reef_name.text</i></b> containing metadata.  The format must be:<br>\n",
    "<blockquote>Coordinates: <p>\n",
    "[min_lat, max_lat, min_lon, max_lon] <br><br>H5 Files:<br>List of all h5 files in folder H5, one file per line.</blockquote></blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Input metadata for reef processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[run.py] <br>\n",
    "Setting reef processing parameters can be done programatically via Karan's top-level processing script <i><b>run.py</i></b>, which updates <i><b>config/data-params.json</i></b>. Here these parameters are set via the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reef_name = 'nasau'\n",
    "data_dir = '/Users/bonnieludka/Spaceship/IceSAT2/run_bathymetry/data/'\n",
    "start_date = '20181101'\n",
    "end_date = '20200601'\n",
    "redownload_is2 = False  #False = use the data already downloaded into the directory\n",
    "earthdata_login = 'bludka'\n",
    "earthdata_password = 'Gnidlaps10!'\n",
    "world_tide_API_key = 'fee8ff39-48eb-42a7-bcc5-3819fce3c1e4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.) Initialize Coral_Reef( ) data object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[run.py] <br>\n",
    "Create a Coral_Reef object to hold metadata for each reef. Take a look in Coral_Reef.py for the methods and variables defined for the Coral_Reef class. Also note that the get_bounding_box( ) method queries <i><b>/data/reef_name/reef_name.txt</i></b> to grab the reef bounding box coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reef = coral_reef.Coral_Reef(data_dir, reef_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.) Get ICESat-2 data for reef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[run.py] <br>\n",
    "This requires running <i><b>/src/IS2_file.main()</i></b>, which we are not going to do for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.) Initialize is2_file( ) data object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[run.py / Depth_profile.py / get_depths(reef)]<br>\n",
    "Assuming for now that the ICESat-2 data have been downloaded and dropped into the <i><b>H5</i></b> directory, we will work with the <i><b>first</i></b> file for the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Identify first ICESat-2 HDF5 file\n",
    "h5_dir = os.path.join(reef.get_path(),'H5')\n",
    "h5_fn = [f for f in os.listdir(h5_dir) if not f.startswith('.')]\n",
    "h5_fn.sort()\n",
    "h5_fn = h5_fn[0]\n",
    "print(h5_dir)\n",
    "print(h5_fn)\n",
    "\n",
    "#Inititalize ICESat-2 file object\n",
    "is2 = is2File.IS2_file(h5_dir, h5_fn, reef.bbox_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.) Estimate photon depths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[run_bathymetry.py / get_depths() / process_H5()]<br>\n",
    "The depth.process_H5(reef) method will run the entire depth estimation workflow. Here it is line-by-line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get paths to all needed directories from reef object\n",
    "icesat_fp, proc_fp, images_fp,data_plots_path = reef.get_file_drectories()\n",
    "reef_name = reef.get_reef_name()\n",
    "\n",
    "#Identify strong ICESat-2 beams\n",
    "strong_beams = is2.get_strong_lasers()\n",
    "print('These are the strong beams for this file: {}'.format(strong_beams))\n",
    "is2_file_tag = is2.get_file_tag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(is2_plot)\n",
    "importlib.reload(water_level)\n",
    "importlib.reload(depth)\n",
    "\n",
    "for laser in ['gt1r', 'gt1l', 'gt2r', 'gt2l', 'gt3r', 'gt3l']:\n",
    "  \n",
    "    #Output directory for csv file containing raw photon data\n",
    "    photon_fn = '{reef_name}_photons_{h5_fn}_{laser}.csv'.format(reef_name=reef_name, h5_fn=is2_file_tag, laser=laser)\n",
    "    photons_path = os.path.join(icesat_fp, photon_fn)\n",
    "    print(\"\\n\" + photons_path)\n",
    "    \n",
    "    \n",
    "    #Load raw photon data if it already exists, else extract it from h5 file\n",
    "    if not os.path.exists(photons_path):\n",
    "\n",
    "        #From process_h5(): \n",
    "        #df = depth.convert_h5_to_csv(is2, laser, photons_path)\n",
    "        \n",
    "        #Create dataframe with photon data\n",
    "        photon_data = is2.get_photon_data(laser)\n",
    "        \n",
    "        h5 = h5py.File(is2.h5_file,'r')\n",
    "        ##returns list of photon height, lat,lon and photon confidence\n",
    "        height = h5.get(laser + '/heights/h_ph')\n",
    "        lat = h5.get(laser + '/heights/lat_ph')\n",
    "        lon = h5.get(laser + '/heights/lon_ph')\n",
    "        conf = h5.get(laser + '/heights/signal_conf_ph')\n",
    "        #print(conf)\n",
    "        #print(is2.h5_file)\n",
    "        #print(h5)\n",
    "        confidence = np.array(conf)\n",
    "        #print(confidence)\n",
    "        #print( laser + '/heights/signal_conf_ph')\n",
    "        #print(height)\n",
    "        conf_land = confidence[:,0]\n",
    "        conf_ocean = confidence[:,1]\n",
    "        onf_inlandwater = confidence[:,3] \n",
    "        \n",
    "        \n",
    "        df_laser = depth.create_photon_df(photon_data)\n",
    "        print(df_laser)\n",
    "\n",
    "        #Clip dataframe to locations within the reef bounding box\n",
    "        coords = is2.get_bbox_coordinates()\n",
    "        min_longitude,min_latitude,max_longitude,max_latitude = coords\n",
    "        df = df_laser.loc[(df_laser.Longitude > min_longitude) & (df_laser.Longitude < max_longitude) &\\\n",
    "            (df_laser.Latitude > min_latitude) & (df_laser.Latitude < max_latitude)]\n",
    "\n",
    "        #If there are photons in the new clipped dataframe\n",
    "        if len(df) != 0:    \n",
    "            \n",
    "            #Unpack the confidence array to individual columns (now done in get_photon_data\n",
    "            #df = depth.individual_confidence(df)\n",
    "            f = water_level.get_water_level(df)\n",
    "            #is2_plot.plot_is2_depths(df, is2, laser, f, [0, 55], 'Raw Photon Elevs')\n",
    "         \n",
    "            #Adjust elevations to reference local sea level (currently this step ALSO subsets on confidence==4)\n",
    "            #Unlike Karan's code, df now contains 'sea_level', which is what has been removed from 'Heights'\n",
    "            df,f = water_level.normalise_sea_level(df)\n",
    "            is2.set_sea_level_function(f,laser)\n",
    "            if len(df) == 0:\n",
    "                print('No photons')\n",
    "                continue\n",
    "\n",
    "            #Adjust depth for speed of light in water\n",
    "            df = water_level.adjust_for_speed_of_light_in_water(df)\n",
    "            #is2_plot.plot_is2_depths(df, is2, laser, 'False', [-25, 5], 'Adjusted Photon Elevs')\n",
    "\n",
    "            #Write a dataframe containing just the photon data in bounding box that meet the confidence criterion\n",
    "            print('Writing photons for clipped dataframe to csv file for {}'.format(laser))\n",
    "            df.to_csv(photons_path)\n",
    "        \n",
    "        else:\n",
    "            print('There are no photons in the clipped dataframe')\n",
    "            \n",
    "    else:\n",
    "        print('Reading photons from prior csv file for {}'.format(laser))\n",
    "        df = pd.read_csv(photons_path)\n",
    "        is2.metadata = is2.load_json()\n",
    "        \n",
    "    #if no photons over reef for this laser, move onto next laser    \n",
    "    if len(df) == 0:\n",
    "        continue\n",
    "\n",
    "    #Calculate predicted depths\n",
    "    print('Number of ICESAT-2 Photons in {laser} is {reef_length}'.format(laser=laser, reef_length=str(len(df))))\n",
    "    depths_fn = '{reef_name}_{h5_fn}_{laser}.csv'.format(reef_name=reef_name,h5_fn=is2_file_tag,laser=laser)\n",
    "    processed_output_path = os.path.join(proc_fp,depths_fn)\n",
    " \n",
    "    if laser in strong_lasers:\n",
    "        eps_val = 1\n",
    "        min_samp = 6\n",
    "    else:\n",
    "        eps_val = 1\n",
    "        min_samp = 5\n",
    "    \n",
    "    bathy = depth.apply_DBSCAN(df, processed_output_path,is2, laser, eps_val, min_samp)\n",
    "    is2_plot.plot_is2_depths_bathy(df, is2, laser, bathy, 'False', [-25, 5], 'Corrected Bathymetric Profile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing: Get tides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = datetime.utcfromtimestamp(0)\n",
    "dt = is2.get_date()\n",
    "ut = (dt - epoch).total_seconds()\n",
    "print(epoch)\n",
    "print(dt)\n",
    "print(dt - epoch)\n",
    "print(ut)\n",
    "\n",
    "bbox = is2.bbox_coordinates\n",
    "print(bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tide_level = tide.get_tide(is2.bbox_coordinates, is2.get_date())\n",
    "min_longitude,min_latitude,max_longitude,max_latitude = is2.bbox_coordinates\n",
    "lat = str((min_latitude+max_latitude)/2)\n",
    "lon = str((min_longitude+max_longitude)/2)\n",
    "stadist = 100\n",
    "base_url = 'https://www.worldtides.info/api/v2?heights'\n",
    "\n",
    "tide_datum = 'MTL'\n",
    "query_string = '&lat={lat}&lon={lon}&start={ut}&datum={datum}&key={api_key}&station_distance={stadist}'\\\n",
    "    .format(lat = lat,lon = lon,ut = ut,datum = tide_datum,api_key = tide.get_API_key(),stadist=stadist)\n",
    "print(query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = base_url + query_string\n",
    "#hitting API and storing contents in json format\n",
    "r = requests.get(url)\n",
    "tide = json.loads(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tide['heights'][0:1])\n",
    "print(tide['responseLat'])\n",
    "print(tide['responseLon'])\n",
    "print(tide['atlas'])\n",
    "print(tide['responseLat'])\n",
    "print(tide['responseLon'])\n",
    "print(tide['responseDatum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
