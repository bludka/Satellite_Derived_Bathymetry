{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document covers the transformation of ICESat-2 photon data over a target reef to depth estimates that will be used to calibrate Sentinel-2 bathymetry estimates.  The relevant program files are located in the <i><b>/src</i></b> directory in Karan Sunil's \"Coral-Reef-Bathymetry\" distribution on GitHub: https://github.com/karans04/Coral-Reef-Bathymetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, make sure this notebook is located and launched from a working directory containing <i><b>Depth_profile.py</i></b> and the other python files in Karan's distribution. After the imports below, any function available in <i><b>Depth_profile.py</i></b> can be called from this notebook with the syntax: depth.function_name(parameters). The same goes for the other imported python files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyproj as proj\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import Depth_profile as depth\n",
    "import Coral_Reef as coral_reef\n",
    "import ICESAT_plots as is2_plot\n",
    "import IS2_file as is2File\n",
    "import Tide_API as tide\n",
    "import Water_level as water_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Create directories and files needed for depth processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In working directory, create a folder called <i><b>data</i></b> and create subfolders for each reef being analyzed.  Within each subfolder, provide the following folders/files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote><b>A.)</b> a GeoJSON file containing the outline of the reef, named <i><b>reef_name.geojson</i></b>.  This file is obtained from http://geojson.io/#map=2/20.0/0.0.  Switch to \"OSM model\" in bottom left corner of window, mark the boundary of reef with the cursor, save points using the menu at upper left (save->GeoJSON), then rename map.geojson file and move to data directory.\n",
    "<p>\n",
    "<b>B.)</b> a folder named <i><b>H5</i></b> which contains ICESat-2 ATL03 data files for this reef.  These are HDF5 files that can be obtained from OpenAltimetry (http://www.openaltimetry.org) or from NASA EarthData search (https://search.earthdata.nasa.gov/search/granules?p=C1705401930-NSIDC_ECS)\n",
    "<p>\n",
    "<b>C.)</b> a file called <i><b>reef_name.text</i></b> containing metadata.  The format must be:<br>\n",
    "<blockquote>Coordinates: <p>\n",
    "[min_lat, max_lat, min_lon, max_lon] <br><br>H5 Files:<br>List of all h5 files in folder H5, one file per line.</blockquote></blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Input metadata for reef processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[run.py] <br>\n",
    "Setting reef processing parameters can be done programatically via Karan's top-level processing script <i><b>run.py</i></b>, which updates <i><b>config/data-params.json</i></b>. Here these parameters are set via the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reef_name = 'nasau'\n",
    "data_dir = '/Users/aborsa/Dropbox/research/bathymetry/analysis/run_bathymetry/data/'\n",
    "start_date = '20181101'\n",
    "end_date = '20200601'\n",
    "redownload_is2 = False  #False = use the data already downloaded into the directory\n",
    "earthdata_login = 'aborsa'\n",
    "earthdata_password = ''\n",
    "world_tide_API_key = 'fee8ff39-48eb-42a7-bcc5-3819fce3c1e4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.) Initialize Coral_Reef( ) data object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[run.py] <br>\n",
    "Create a Coral_Reef object to hold metadata for each reef. Take a look in Coral_Reef.py for the methods and variables defined for the Coral_Reef class. Also note that the get_bounding_box( ) method queries <i><b>/data/reef_name/reef_name.txt</i></b> to grab the reef bounding box coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reef = coral_reef.Coral_Reef(data_dir, reef_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.) Get ICESat-2 data for reef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[run.py] <br>\n",
    "This requires running <i><b>/src/IS2_file.main()</i></b>, which we are not going to do for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.) Initialize is2_file( ) data object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[run.py / Depth_profile.py / get_depths(reef)]<br>\n",
    "Assuming for now that the ICESat-2 data have been downloaded and dropped into the <i><b>H5</i></b> directory, we will work with the <i><b>first</i></b> file for the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aborsa/Dropbox/research/bathymetry/analysis/run_bathymetry/data/nasau/H5\n",
      "processed_ATL03_20181210123712_11130108_003_01.h5\n"
     ]
    }
   ],
   "source": [
    "#Identify first ICESat-2 HDF5 file\n",
    "h5_dir = os.path.join(reef.get_path(),'H5')\n",
    "h5_fn = [f for f in os.listdir(h5_dir) if not f.startswith('.')]\n",
    "h5_fn.sort()\n",
    "h5_fn = h5_fn[1]\n",
    "print(h5_dir)\n",
    "print(h5_fn)\n",
    "\n",
    "#Inititalize ICESat-2 file object\n",
    "is2 = is2File.IS2_file(h5_dir, h5_fn, reef.bbox_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.) Estimate photon depths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[run_bathymetry.py / get_depths() / process_H5()]<br>\n",
    "The depth.process_H5(reef) method will run the entire depth estimation workflow. Here it is line-by-line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the strong beams for this file: ['gt1r', 'gt2r', 'gt3r']\n"
     ]
    }
   ],
   "source": [
    "#Get paths to all needed directories from reef object\n",
    "icesat_fp, proc_fp, images_fp,data_plots_path = reef.get_file_drectories()\n",
    "reef_name = reef.get_reef_name()\n",
    "\n",
    "#Loop over the three strong ICESat-2 beams\n",
    "print('These are the strong beams for this file: {}'.format(is2.get_strong_lasers()))\n",
    "is2_file_tag = is2.get_file_tag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt1r\n",
      "/Users/aborsa/Dropbox/research/bathymetry/analysis/run_bathymetry/data/nasau/Output/Data_Cleaning/ICESAT_photons/nasau_photons_processed_ATL03_20181210123712_11130108_003_01_gt1r.csv\n"
     ]
    }
   ],
   "source": [
    "# for laser in is2.get_strong_lasers():\n",
    "# just test one laser for the moment\n",
    "for laser in ['gt1r']:\n",
    "    print(laser)\n",
    "    \n",
    "    #Output directory for csv file containing raw photon data\n",
    "    photon_fn = '{reef_name}_photons_{h5_fn}_{laser}.csv'.format(reef_name=reef_name, h5_fn=is2_file_tag, laser=laser)\n",
    "    photons_path = os.path.join(icesat_fp, photon_fn)\n",
    "    print(photons_path)\n",
    "    \n",
    "    #Load raw photon data if it already exists, else extract it from h5 file\n",
    "    if not os.path.exists(photons_path):\n",
    "\n",
    "        #From process_h5(): df = convert_h5_to_csv(is2_file, laser, photons_path)\n",
    "        \n",
    "        #Create dataframe with photon data\n",
    "        photon_data = is2.get_photon_data(laser)\n",
    "        df_laser = depth.create_photon_df(photon_data)\n",
    "\n",
    "        #Clip dataframe to locations within the reef bounding box\n",
    "        coords = is2.get_bbox_coordinates()\n",
    "        min_longitude,min_latitude,max_longitude,max_latitude = coords\n",
    "        df = df_laser.loc[(df_laser.Longitude > min_longitude) & (df_laser.Longitude < max_longitude) &\\\n",
    "            (df_laser.Latitude > min_latitude) & (df_laser.Latitude < max_latitude)]\n",
    "\n",
    "        #If there are photons in the new clipped dataframe\n",
    "        if len(df) != 0:    \n",
    "            \n",
    "            #Unpack the confidence array to individual columns\n",
    "            #df = depth.individual_confidence(df)\n",
    "            print(df)\n",
    "            print('unpack confidence array')\n",
    "            \n",
    "            #Adjust elevations to reference local sea level (currently this step ALSO subsets on confidence)\n",
    "            df,f = water_level.normalise_sea_level(df)\n",
    "            if len(df) == 0:\n",
    "                print('No photons')\n",
    "                continue\n",
    "            print('adjust elevations to reference sea level')\n",
    "            \n",
    "            #Adjust depth for speed of light in water (note use of alternative method in Water_level.py)\n",
    "            is2.set_sea_level_function(f,laser)\n",
    "            df = water_level.adjust_for_refractive_index(df)\n",
    "            print('adjust depth for speed of light in water')\n",
    "            \n",
    "            #Write a dataframe containing just the photon data in bounding box that meet the confidence criterion\n",
    "            df.to_csv(photons_path)\n",
    "            print('write datafrane to .csv file')\n",
    "            \n",
    "    else:\n",
    "        df = pd.read_csv(photons_path)\n",
    "        is2.metadata = is2.load_json()\n",
    "\n",
    "    #if no photons over reef for this laser, move onto next laser    \n",
    "    if len(df) == 0:\n",
    "        continue\n",
    "\n",
    "    print('Number of ICESAT-2 Photons in {laser} is {reef_length}'.format(laser=laser, reef_length=str(len(photons))))\n",
    "    #Calculate predicted depths and saves file to the following path\n",
    "    depths_fn = '{reef_name}_{h5_fn}_{laser}.csv'.format(reef_name=reef_name,h5_fn=is2_file_tag,laser=laser)\n",
    "    processed_output_path = os.path.join(proc_fp,depths_fn)\n",
    "    #bathy = depth.depth_profile_adaptive(photons,processed_output_path,is2,laser)\n",
    "    bathy = depth.apply_DBSCAN(df,processed_output_path,is2,laser)\n",
    "\n",
    "    print('Number of reef Photons in {laser} after cleaning is {reef_length}'.format(laser=laser, reef_length=str(len(bathy))))\n",
    "    if len(bathy) != 0:\n",
    "        #combines ICESAT-2 output with depth predictions\n",
    "        out_df = depth.combine_is2_reef(df, bathy)\n",
    "        data_plots_fn = '{reef_name}_{h5_fn}_{laser}_plots.csv'.format(reef_name=reef_name,h5_fn=is2_file_tag,laser=laser)\n",
    "        out_df.to_csv(os.path.join(data_plots_path,data_plots_fn))\n",
    "        #plot predicted depths with ICESAT photons\n",
    "        is2_plot.p_is2(out_df,is2,laser,images_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Height Latitude Longitude         Confidence\n",
      "3145   52.0714 -18.6238  -178.459  [4, 4, -1, -1, 4]\n",
      "3146   25.9332 -18.6238  -178.459  [2, 0, -1, -1, 0]\n",
      "3147   52.2668 -18.6238  -178.459  [4, 4, -1, -1, 4]\n",
      "3148   51.7941 -18.6238  -178.459  [4, 4, -1, -1, 4]\n",
      "3149   52.3271 -18.6238  -178.459  [4, 4, -1, -1, 4]\n",
      "...        ...      ...       ...                ...\n",
      "60282  51.7596 -18.7552  -178.473  [4, 4, -1, -1, 4]\n",
      "60283  51.8081 -18.7552  -178.473  [4, 4, -1, -1, 4]\n",
      "60284  51.9158 -18.7552  -178.473  [4, 4, -1, -1, 4]\n",
      "60285  51.8076 -18.7552  -178.473  [4, 4, -1, -1, 4]\n",
      "60286  51.8122 -18.7552  -178.473  [4, 4, -1, -1, 4]\n",
      "\n",
      "[57142 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(is2.sea_level_func['gt1r'])\n",
    "print(is2.metadata)\n",
    "\n",
    "print(np.poly1d(is2.load_json()[is2.h5_fn]['sea_level_func'][laser]))\n",
    "print(np.poly1d(is2.sea_level_func[laser]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing: Get tides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = datetime.utcfromtimestamp(0)\n",
    "dt = is2.get_date()\n",
    "ut = (dt - epoch).total_seconds()\n",
    "print(epoch)\n",
    "print(dt)\n",
    "print(dt - epoch)\n",
    "print(ut)\n",
    "\n",
    "bbox = is2.bbox_coordinates\n",
    "print(bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tide_level = tide.get_tide(is2.bbox_coordinates, is2.get_date())\n",
    "min_longitude,min_latitude,max_longitude,max_latitude = is2.bbox_coordinates\n",
    "lat = str((min_latitude+max_latitude)/2)\n",
    "lon = str((min_longitude+max_longitude)/2)\n",
    "stadist = 100\n",
    "base_url = 'https://www.worldtides.info/api/v2?heights'\n",
    "\n",
    "tide_datum = 'MTL'\n",
    "query_string = '&lat={lat}&lon={lon}&start={ut}&datum={datum}&key={api_key}&station_distance={stadist}'\\\n",
    "    .format(lat = lat,lon = lon,ut = ut,datum = tide_datum,api_key = tide.get_API_key(),stadist=stadist)\n",
    "print(query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = base_url + query_string\n",
    "#hitting API and storing contents in json format\n",
    "r = requests.get(url)\n",
    "tide = json.loads(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tide['heights'][0:1])\n",
    "print(tide['atlas'])\n",
    "print(tide['responseLat'])\n",
    "print(tide['responseLon'])\n",
    "print(tide['responseDatum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tide['heights'][0:1])\n",
    "print(tide['atlas'])\n",
    "print(tide['responseLat'])\n",
    "print(tide['responseLon'])\n",
    "print(tide['responseDatum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tide['heights'][0:1])\n",
    "print(tide['responseLat'])\n",
    "print(tide['responseLon'])\n",
    "print(tide['atlas'])\n",
    "print(tide['responseLat'])\n",
    "print(tide['responseLon'])\n",
    "print(tide['responseDatum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
